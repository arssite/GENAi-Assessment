{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnasEGg9zKKYouIa/fcK90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arssite/GENAi-Assessment/blob/main/TranscriptSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch gradio youtube_transcript_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibxV-pRbrnZ6",
        "outputId": "d6692f6c-5365-4efc-a64e-96a2bec93cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m230.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, youtube_transcript_api, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.9 semantic-version-2.10.0 starlette-0.38.6 tomlkit-0.12.0 uvicorn-0.31.0 websockets-12.0 youtube_transcript_api-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "ZGXHmKHrrhsT",
        "outputId": "c2d7b810-d248-4dfa-a52d-92d076bf30a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://fc4fbab3bfcfd53599.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fc4fbab3bfcfd53599.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import re\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api.formatters import TextFormatter\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "text_summary = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "# model_path = (\"../Models/models--sshleifer--distilbart-cnn-12-6/snapshots\"\n",
        "#               \"/a4f8f3ea906ed274767e9906dbaede7531d660ff\")\n",
        "# text_summary = pipeline(\"summarization\", model=model_path,\n",
        "#                 torch_dtype=torch.bfloat16)\n",
        "\n",
        "def summary (input):\n",
        "    output = text_summary(input)\n",
        "    if not output or len(output) == 0:\n",
        "        return \"No summary could be generated. Please check the input.\"\n",
        "    return output[0]['summary_text']\n",
        "\n",
        "\n",
        "\n",
        "def extract_video_id(url):\n",
        "    # Regex to extract the video ID from various YouTube URL formats\n",
        "    regex = r\"(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|\\S*?[?&]v=)|youtu\\.be\\/)([a-zA-Z0-9_-]{11})\"\n",
        "    match = re.search(regex, url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_youtube_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Video ID could not be extracted.\"\n",
        "\n",
        "    try:\n",
        "        # Fetch the transcript\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "        # Format the transcript into plain text\n",
        "        formatter = TextFormatter()\n",
        "        text_transcript = formatter.format_transcript(transcript)\n",
        "        summary_text = summary(text_transcript)\n",
        "\n",
        "        return summary_text\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "gr.close_all()\n",
        "demo = gr.Interface(fn=get_youtube_transcript,\n",
        "                    inputs=[gr.Textbox(label=\"Input YouTube Url to summarize\",lines=1)],\n",
        "                    outputs=[gr.Textbox(label=\"Summarized text\",lines=4)],\n",
        "                    title=\"Transcript Summarizer\",\n",
        "                    description=\"THIS APPLICATION WILL BE USED TO SUMMARIZE THE VIDEO SCRIPT.\")\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api.formatters import TextFormatter\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "text_summary = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "def summary(input):\n",
        "    output = text_summary(input)\n",
        "\n",
        "    # Check if the model returned a result\n",
        "    if not output or len(output) == 0:\n",
        "        return \"No summary could be generated. Please check the input.\"\n",
        "\n",
        "    return output[0]['summary_text']\n",
        "\n",
        "def extract_video_id(url):\n",
        "    regex = r\"(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|\\S*?[?&]v=)|youtu\\.be\\/)([a-zA-Z0-9_-]{11})\"\n",
        "    match = re.search(regex, url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "def get_youtube_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Video ID could not be extracted.\"\n",
        "\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        formatter = TextFormatter()\n",
        "        text_transcript = formatter.format_transcript(transcript)\n",
        "        summary_text = summary(text_transcript)\n",
        "        return summary_text\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "gr.close_all()\n",
        "demo = gr.Interface(fn=get_youtube_transcript,\n",
        "                    inputs=[gr.Textbox(label=\"Input YouTube URL to summarize\", lines=1)],\n",
        "                    outputs=[gr.Textbox(label=\"Summarized text\", lines=4)],\n",
        "                    title=\"Transcript Summarizer\",\n",
        "                    description=\"This application summarizes YouTube video transcripts.\")\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "yhU0xG4yrwSd",
        "outputId": "d2060127-5118-43fd-8a60-6be6fedb9a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://a42504cbde5621dc93.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a42504cbde5621dc93.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://youtu.be/O7ZYJIHRs10?si=2jjyfi_cNCsNa4Ir\"\n",
        "video_id = extract_video_id(url)\n",
        "print(video_id)  # Expected output: l00VBUXl1Q4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt257RdQtuXW",
        "outputId": "8a96c07e-89b9-4e9f-f3d6-32890f642181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O7ZYJIHRs10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "try:\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(\"O7ZYJIHRs10\")\n",
        "    print(transcript)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N9GjqmAuJyi",
        "outputId": "e2d42a24-a38d-41bf-94de-da5e7036812d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': '[Music]', 'start': 0.44, 'duration': 3.209}, {'text': 'code diggers a branch of rs site code', 'start': 7.52, 'duration': 5.44}, {'text': 'digger helps to reduce complexity of', 'start': 10.8, 'duration': 3.28}, {'text': 'code and', 'start': 12.96, 'duration': 4.079}, {'text': 'generate new idea for algorithm', 'start': 14.08, 'duration': 5.119}, {'text': 'so stay with us', 'start': 17.039, 'duration': 3.521}, {'text': 'hello boss', 'start': 19.199, 'duration': 4.0}, {'text': \"so today's we learn to reverse number\", 'start': 20.56, 'duration': 3.76}, {'text': 'alphabet', 'start': 23.199, 'duration': 3.121}, {'text': 'word or symbol', 'start': 24.32, 'duration': 5.23}, {'text': 'let code', 'start': 26.32, 'duration': 34.08}, {'text': '[Music]', 'start': 29.55, 'duration': 30.85}, {'text': 'so we take two variable', 'start': 61.12, 'duration': 5.28}, {'text': 'user for taking any input from user cast', 'start': 63.199, 'duration': 4.161}, {'text': 'is for', 'start': 66.4, 'duration': 3.759}, {'text': 'if someone give any integer value then', 'start': 67.36, 'duration': 5.68}, {'text': 'it convert into string as indexing is', 'start': 70.159, 'duration': 6.64}, {'text': 'not possible in integer or float', 'start': 73.04, 'duration': 8.68}, {'text': \"move ahead let's see some outputs\", 'start': 76.799, 'duration': 4.921}, {'text': \"code diggers a branch of r's site dig\", 'start': 86.159, 'duration': 4.64}, {'text': 'code dig big', 'start': 89.439, 'duration': 6.121}, {'text': \"don't forget to subscribe rs site\", 'start': 90.799, 'duration': 4.761}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api.formatters import TextFormatter\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the summarization model\n",
        "text_summary = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "def summary(input_text):\n",
        "    # Summarize in chunks to avoid token limit errors\n",
        "    max_chunk_size = 1024\n",
        "\n",
        "    def split_text(text, max_len):\n",
        "        sentences = text.split('. ')\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        for sentence in sentences:\n",
        "            sentence_length = len(sentence.split())\n",
        "            if current_length + sentence_length > max_len:\n",
        "                yield ' '.join(current_chunk)\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "        if current_chunk:\n",
        "            yield ' '.join(current_chunk)\n",
        "\n",
        "    chunks = list(split_text(input_text, max_chunk_size))\n",
        "\n",
        "    summarized_chunks = []\n",
        "    for chunk in chunks:\n",
        "        output = text_summary(chunk)\n",
        "        if output and len(output) > 0:\n",
        "            summarized_chunks.append(output[0]['summary_text'])\n",
        "\n",
        "    return ' '.join(summarized_chunks)\n",
        "\n",
        "# Extract the YouTube video ID from the URL\n",
        "def extract_video_id(url):\n",
        "    regex = r\"(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|\\S*?[?&]v=)|youtu\\.be\\/)([a-zA-Z0-9_-]{11})\"\n",
        "    match = re.search(regex, url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "# Fetch and summarize the YouTube transcript\n",
        "def get_youtube_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Video ID could not be extracted.\"\n",
        "\n",
        "    try:\n",
        "        # Fetch the transcript\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "        # Format the transcript into plain text\n",
        "        formatter = TextFormatter()\n",
        "        text_transcript = formatter.format_transcript(transcript)\n",
        "\n",
        "        # Summarize the text transcript\n",
        "        summary_text = summary(text_transcript)\n",
        "\n",
        "        return summary_text\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "gr.close_all()\n",
        "demo = gr.Interface(\n",
        "    fn=get_youtube_transcript,\n",
        "    inputs=[gr.Textbox(label=\"Input YouTube URL to summarize\", lines=1)],\n",
        "    outputs=[gr.Textbox(label=\"Summarized text\", lines=4)],\n",
        "    title=\"Transcript Summarizer\",\n",
        "    description=\"This application summarizes YouTube video transcripts.\"\n",
        ")\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "HAeMxC7FuPWu",
        "outputId": "6a6ecef9-6141-456a-eee8-172a5ba57d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ace9e1f4f0772108f9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ace9e1f4f0772108f9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api.formatters import TextFormatter\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the summarization model\n",
        "text_summary = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "# Summarization function that handles text chunks\n",
        "def summary(input_text):\n",
        "    # Limit input length to prevent exceeding token limits\n",
        "    max_chunk_size = 1024  # Token limit for the model\n",
        "\n",
        "    # Function to split text into chunks of approximately max_chunk_size\n",
        "    def split_text(text, max_len):\n",
        "        sentences = text.split('. ')\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        for sentence in sentences:\n",
        "            sentence_length = len(sentence.split())  # Get word count\n",
        "            if current_length + sentence_length > max_len:\n",
        "                yield ' '.join(current_chunk)\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "        if current_chunk:\n",
        "            yield ' '.join(current_chunk)\n",
        "\n",
        "    # Split transcript into smaller chunks\n",
        "    chunks = list(split_text(input_text, max_chunk_size))\n",
        "\n",
        "    # Summarize each chunk and combine the results\n",
        "    summarized_chunks = []\n",
        "    for chunk in chunks:\n",
        "        output = text_summary(chunk)\n",
        "        if output and len(output) > 0:\n",
        "            summarized_chunks.append(output[0]['summary_text'])\n",
        "\n",
        "    # Combine summarized chunks into final output\n",
        "    return ' '.join(summarized_chunks)\n",
        "\n",
        "# Extract the YouTube video ID from the URL\n",
        "def extract_video_id(url):\n",
        "    regex = r\"(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|\\S*?[?&]v=)|youtu\\.be\\/)([a-zA-Z0-9_-]{11})\"\n",
        "    match = re.search(regex, url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "# Fetch and summarize the YouTube transcript\n",
        "def get_youtube_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Video ID could not be extracted.\"\n",
        "\n",
        "    try:\n",
        "        # Fetch the transcript\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "        # Format the transcript into plain text\n",
        "        formatter = TextFormatter()\n",
        "        text_transcript = formatter.format_transcript(transcript)\n",
        "\n",
        "        # Summarize the text transcript\n",
        "        summary_text = summary(text_transcript)\n",
        "\n",
        "        return summary_text\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "# Launch Gradio Interface\n",
        "gr.close_all()\n",
        "demo = gr.Interface(\n",
        "    fn=get_youtube_transcript,\n",
        "    inputs=[gr.Textbox(label=\"Input YouTube URL to summarize\", lines=1)],\n",
        "    outputs=[gr.Textbox(label=\"Summarized text\", lines=4)],\n",
        "    title=\"Transcript Summarizer\",\n",
        "    description=\"This application summarizes YouTube video transcripts.\"\n",
        ")\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "i_BxbmJTuY1b",
        "outputId": "731dfea0-8d28-4c5d-9922-ba98be3d85c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://c389413e74503c2f6f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c389413e74503c2f6f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api.formatters import TextFormatter\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the summarization model with max_length and min_length defined\n",
        "text_summary = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "# Function to generate a summary restricted to 1000 tokens\n",
        "def summary(input_text):\n",
        "    max_chunk_size = 1024  # Define max input size for each chunk (model limit)\n",
        "    max_summary_tokens = 1000  # Restrict the summary to 1000 tokens\n",
        "\n",
        "    # Split text into smaller chunks\n",
        "    def split_text(text, max_len):\n",
        "        sentences = text.split('. ')\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        for sentence in sentences:\n",
        "            sentence_length = len(sentence.split())\n",
        "            if current_length + sentence_length > max_len:\n",
        "                yield ' '.join(current_chunk)\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "        if current_chunk:\n",
        "            yield ' '.join(current_chunk)\n",
        "\n",
        "    # Summarize each chunk and combine the results\n",
        "    chunks = list(split_text(input_text, max_chunk_size))\n",
        "    summarized_chunks = []\n",
        "\n",
        "    total_token_count = 0\n",
        "\n",
        "    for chunk in chunks:\n",
        "        output = text_summary(chunk, max_length=max_summary_tokens, min_length=30)  # Adjust the length\n",
        "        if output and len(output) > 0:\n",
        "            summary_text = output[0]['summary_text']\n",
        "            summary_token_count = len(summary_text.split())\n",
        "            total_token_count += summary_token_count\n",
        "\n",
        "            # Stop if we've reached 1000 tokens\n",
        "            if total_token_count >= max_summary_tokens:\n",
        "                remaining_tokens = max_summary_tokens - (total_token_count - summary_token_count)\n",
        "                truncated_summary = ' '.join(summary_text.split()[:remaining_tokens])\n",
        "                summarized_chunks.append(truncated_summary)\n",
        "                break\n",
        "            else:\n",
        "                summarized_chunks.append(summary_text)\n",
        "\n",
        "    # Combine summarized chunks into the final output\n",
        "    return ' '.join(summarized_chunks)\n",
        "\n",
        "# Extract the YouTube video ID from the URL\n",
        "def extract_video_id(url):\n",
        "    regex = r\"(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|\\S*?[?&]v=)|youtu\\.be\\/)([a-zA-Z0-9_-]{11})\"\n",
        "    match = re.search(regex, url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "# Fetch and summarize the YouTube transcript\n",
        "def get_youtube_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Video ID could not be extracted.\"\n",
        "\n",
        "    try:\n",
        "        # Fetch the transcript\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "        # Format the transcript into plain text\n",
        "        formatter = TextFormatter()\n",
        "        text_transcript = formatter.format_transcript(transcript)\n",
        "\n",
        "        # Summarize the text transcript\n",
        "        summary_text = summary(text_transcript)\n",
        "\n",
        "        return summary_text\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "# Launch Gradio Interface\n",
        "gr.close_all()\n",
        "demo = gr.Interface(\n",
        "    fn=get_youtube_transcript,\n",
        "    inputs=[gr.Textbox(label=\"Input YouTube URL to summarize\", lines=1)],\n",
        "    outputs=[gr.Textbox(label=\"Summarized text\", lines=4)],\n",
        "    title=\"Transcript Summarizer\",\n",
        "    description=\"This application summarizes YouTube video transcripts.\"\n",
        ")\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "7PEbxGvxu1m2",
        "outputId": "1314c4f0-80f8-4a75-a737-604c1ab56088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://1f528b68378146f2df.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1f528b68378146f2df.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdcGyz84vdco"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}